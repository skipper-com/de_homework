{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3307b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51eee3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credentials_location = '/home/skipper/.gc/dtc-de-project-374319-0290d4357a99.json'\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"./lib/gcs-connector-hadoop3-latest.jar.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19884dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 18:17:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/25 18:17:10 WARN DependencyUtils: Local jar /home/skipper/data-engineering-zoomcamp/homework/5_week/lib/gcs-connector-hadoop3-latest.jar.jar does not exist, skipping.\n",
      "23/02/25 18:17:10 INFO SparkContext: Running Spark version 3.3.2\n",
      "23/02/25 18:17:10 INFO ResourceUtils: ==============================================================\n",
      "23/02/25 18:17:10 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/02/25 18:17:10 INFO ResourceUtils: ==============================================================\n",
      "23/02/25 18:17:10 INFO SparkContext: Submitted application: test\n",
      "23/02/25 18:17:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/02/25 18:17:10 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/02/25 18:17:10 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/02/25 18:17:10 INFO SecurityManager: Changing view acls to: skipper\n",
      "23/02/25 18:17:10 INFO SecurityManager: Changing modify acls to: skipper\n",
      "23/02/25 18:17:10 INFO SecurityManager: Changing view acls groups to: \n",
      "23/02/25 18:17:10 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/02/25 18:17:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(skipper); groups with view permissions: Set(); users  with modify permissions: Set(skipper); groups with modify permissions: Set()\n",
      "23/02/25 18:17:10 INFO Utils: Successfully started service 'sparkDriver' on port 40669.\n",
      "23/02/25 18:17:10 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/02/25 18:17:10 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/02/25 18:17:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/02/25 18:17:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/02/25 18:17:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/02/25 18:17:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3b3e15c6-80d4-4495-ad03-5d4b7e9e9520\n",
      "23/02/25 18:17:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/02/25 18:17:11 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/02/25 18:17:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/02/25 18:17:11 ERROR SparkContext: Failed to add ./lib/gcs-connector-hadoop3-latest.jar.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/skipper/data-engineering-zoomcamp/homework/5_week/lib/gcs-connector-hadoop3-latest.jar.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1959)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2014)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:67)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "23/02/25 18:17:11 INFO Executor: Starting executor ID driver on host dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal\n",
      "23/02/25 18:17:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/02/25 18:17:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34501.\n",
      "23/02/25 18:17:11 INFO NettyBlockTransferService: Server created on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal:34501\n",
      "23/02/25 18:17:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/02/25 18:17:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, 34501, None)\n",
      "23/02/25 18:17:11 INFO BlockManagerMasterEndpoint: Registering block manager dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal:34501 with 434.4 MiB RAM, BlockManagerId(driver, dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, 34501, None)\n",
      "23/02/25 18:17:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, 34501, None)\n",
      "23/02/25 18:17:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, 34501, None)\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecae63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee1eb1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 18:19:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/02/25 18:19:11 INFO SharedState: Warehouse path is 'file:/home/skipper/data-engineering-zoomcamp/homework/5_week/spark-warehouse'.\n",
      "23/02/25 18:19:12 INFO InMemoryFileIndex: It took 59 ms to list leaf files for 5 paths.\n",
      "23/02/25 18:19:12 INFO SparkContext: Starting job: parquet at DirectMethodHandleAccessor.java:104\n",
      "23/02/25 18:19:12 INFO DAGScheduler: Got job 0 (parquet at DirectMethodHandleAccessor.java:104) with 1 output partitions\n",
      "23/02/25 18:19:12 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at DirectMethodHandleAccessor.java:104)\n",
      "23/02/25 18:19:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/02/25 18:19:12 INFO DAGScheduler: Missing parents: List()\n",
      "23/02/25 18:19:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at DirectMethodHandleAccessor.java:104), which has no missing parents\n",
      "23/02/25 18:19:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.9 KiB, free 434.3 MiB)\n",
      "23/02/25 18:19:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 434.3 MiB)\n",
      "23/02/25 18:19:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal:34501 (size: 37.4 KiB, free: 434.4 MiB)\n",
      "23/02/25 18:19:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "23/02/25 18:19:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at DirectMethodHandleAccessor.java:104) (first 15 tasks are for partitions Vector(0))\n",
      "23/02/25 18:19:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/02/25 18:19:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, executor driver, partition 0, PROCESS_LOCAL, 4688 bytes) taskResourceAssignments Map()\n",
      "23/02/25 18:19:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 18:19:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2050 bytes result sent to driver\n",
      "23/02/25 18:19:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 575 ms on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal (executor driver) (1/1)\n",
      "23/02/25 18:19:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/02/25 18:19:13 INFO DAGScheduler: ResultStage 0 (parquet at DirectMethodHandleAccessor.java:104) finished in 0.797 s\n",
      "23/02/25 18:19:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/02/25 18:19:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/02/25 18:19:13 INFO DAGScheduler: Job 0 finished: parquet at DirectMethodHandleAccessor.java:104, took 0.860352 s\n",
      "23/02/25 18:19:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal:34501 in memory (size: 37.4 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "df_fhv = spark.read.parquet('data/fhv/pq/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ca5ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 18:19:47 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/02/25 18:19:47 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/02/25 18:19:47 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
      "23/02/25 18:19:47 INFO CodeGenerator: Code generated in 27.86389 ms\n",
      "23/02/25 18:19:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 202.5 KiB, free 433.9 MiB)\n",
      "23/02/25 18:19:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 433.9 MiB)\n",
      "23/02/25 18:19:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal:34501 (size: 35.1 KiB, free: 434.3 MiB)\n",
      "23/02/25 18:19:47 INFO SparkContext: Created broadcast 3 from count at DirectMethodHandleAccessor.java:104\n",
      "23/02/25 18:19:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 74739479 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/02/25 18:19:47 INFO DAGScheduler: Registering RDD 9 (count at DirectMethodHandleAccessor.java:104) as input to shuffle 0\n",
      "23/02/25 18:19:47 INFO DAGScheduler: Got map stage job 2 (count at DirectMethodHandleAccessor.java:104) with 4 output partitions\n",
      "23/02/25 18:19:47 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at DirectMethodHandleAccessor.java:104)\n",
      "23/02/25 18:19:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/02/25 18:19:47 INFO DAGScheduler: Missing parents: List()\n",
      "23/02/25 18:19:47 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at count at DirectMethodHandleAccessor.java:104), which has no missing parents\n",
      "23/02/25 18:19:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)\n",
      "23/02/25 18:19:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 433.9 MiB)\n",
      "23/02/25 18:19:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal:34501 (size: 7.3 KiB, free: 434.3 MiB)\n",
      "23/02/25 18:19:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "23/02/25 18:19:47 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at count at DirectMethodHandleAccessor.java:104) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "23/02/25 18:19:47 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0\n",
      "23/02/25 18:19:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, executor driver, partition 0, PROCESS_LOCAL, 5002 bytes) taskResourceAssignments Map()\n",
      "23/02/25 18:19:47 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, executor driver, partition 1, PROCESS_LOCAL, 5002 bytes) taskResourceAssignments Map()\n",
      "23/02/25 18:19:47 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4) (dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, executor driver, partition 2, PROCESS_LOCAL, 5002 bytes) taskResourceAssignments Map()\n",
      "23/02/25 18:19:47 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5) (dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, executor driver, partition 3, PROCESS_LOCAL, 5002 bytes) taskResourceAssignments Map()\n",
      "23/02/25 18:19:47 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "23/02/25 18:19:47 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)\n",
      "23/02/25 18:19:47 INFO Executor: Running task 2.0 in stage 2.0 (TID 4)\n",
      "23/02/25 18:19:47 INFO Executor: Running task 3.0 in stage 2.0 (TID 5)\n",
      "23/02/25 18:19:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal:34501 in memory (size: 35.4 KiB, free: 434.4 MiB)\n",
      "23/02/25 18:19:47 INFO FileScanRDD: Reading File path: file:///home/skipper/data-engineering-zoomcamp/homework/5_week/data/fhv/pq/part-00000-d5f0ea59-622f-4d4e-a91e-7104e092cf5b-c000.snappy.parquet, range: 0-70549774, partition values: [empty row]\n",
      "23/02/25 18:19:47 INFO FileScanRDD: Reading File path: file:///home/skipper/data-engineering-zoomcamp/homework/5_week/data/fhv/pq/part-00002-d5f0ea59-622f-4d4e-a91e-7104e092cf5b-c000.snappy.parquet, range: 0-70543659, partition values: [empty row]\n",
      "23/02/25 18:19:47 INFO FileScanRDD: Reading File path: file:///home/skipper/data-engineering-zoomcamp/homework/5_week/data/fhv/pq/part-00001-d5f0ea59-622f-4d4e-a91e-7104e092cf5b-c000.snappy.parquet, range: 0-70546198, partition values: [empty row]\n",
      "23/02/25 18:19:47 INFO FileScanRDD: Reading File path: file:///home/skipper/data-engineering-zoomcamp/homework/5_week/data/fhv/pq/part-00003-d5f0ea59-622f-4d4e-a91e-7104e092cf5b-c000.snappy.parquet, range: 0-70541071, partition values: [empty row]\n",
      "23/02/25 18:19:47 INFO BlockManagerInfo: Removed broadcast_2_piece0 on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal:34501 in memory (size: 7.2 KiB, free: 434.4 MiB)\n",
      "23/02/25 18:19:47 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2222 bytes result sent to driver\n",
      "23/02/25 18:19:47 INFO Executor: Finished task 3.0 in stage 2.0 (TID 5). 2222 bytes result sent to driver\n",
      "23/02/25 18:19:47 INFO Executor: Finished task 2.0 in stage 2.0 (TID 4). 2222 bytes result sent to driver\n",
      "23/02/25 18:19:47 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 211 ms on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal (executor driver) (1/4)\n",
      "23/02/25 18:19:47 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 216 ms on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal (executor driver) (2/4)\n",
      "23/02/25 18:19:47 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 5) in 213 ms on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal (executor driver) (3/4)\n",
      "23/02/25 18:19:47 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 2222 bytes result sent to driver\n",
      "23/02/25 18:19:47 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 224 ms on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal (executor driver) (4/4)\n",
      "23/02/25 18:19:47 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "23/02/25 18:19:47 INFO DAGScheduler: ShuffleMapStage 2 (count at DirectMethodHandleAccessor.java:104) finished in 0.259 s\n",
      "23/02/25 18:19:47 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/02/25 18:19:47 INFO DAGScheduler: running: Set()\n",
      "23/02/25 18:19:47 INFO DAGScheduler: waiting: Set()\n",
      "23/02/25 18:19:47 INFO DAGScheduler: failed: Set()\n",
      "23/02/25 18:19:48 INFO CodeGenerator: Code generated in 16.677184 ms\n",
      "23/02/25 18:19:48 INFO SparkContext: Starting job: count at DirectMethodHandleAccessor.java:104\n",
      "23/02/25 18:19:48 INFO DAGScheduler: Got job 3 (count at DirectMethodHandleAccessor.java:104) with 1 output partitions\n",
      "23/02/25 18:19:48 INFO DAGScheduler: Final stage: ResultStage 4 (count at DirectMethodHandleAccessor.java:104)\n",
      "23/02/25 18:19:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "23/02/25 18:19:48 INFO DAGScheduler: Missing parents: List()\n",
      "23/02/25 18:19:48 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[12] at count at DirectMethodHandleAccessor.java:104), which has no missing parents\n",
      "23/02/25 18:19:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)\n",
      "23/02/25 18:19:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
      "23/02/25 18:19:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal:34501 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "23/02/25 18:19:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
      "23/02/25 18:19:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[12] at count at DirectMethodHandleAccessor.java:104) (first 15 tasks are for partitions Vector(0))\n",
      "23/02/25 18:19:48 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "23/02/25 18:19:48 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6) (dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "23/02/25 18:19:48 INFO Executor: Running task 0.0 in stage 4.0 (TID 6)\n",
      "23/02/25 18:19:48 INFO ShuffleBlockFetcherIterator: Getting 4 (240.0 B) non-empty blocks including 4 (240.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/02/25 18:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 18:19:48 INFO Executor: Finished task 0.0 in stage 4.0 (TID 6). 2656 bytes result sent to driver\n",
      "23/02/25 18:19:48 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 69 ms on dtc-de-vm-1.europe-west3-c.c.dtc-de-project-374319.internal (executor driver) (1/1)\n",
      "23/02/25 18:19:48 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "23/02/25 18:19:48 INFO DAGScheduler: ResultStage 4 (count at DirectMethodHandleAccessor.java:104) finished in 0.084 s\n",
      "23/02/25 18:19:48 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/02/25 18:19:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "23/02/25 18:19:48 INFO DAGScheduler: Job 3 finished: count at DirectMethodHandleAccessor.java:104, took 0.100382 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14961892"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
